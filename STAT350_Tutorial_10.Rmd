---
title: "STAT350 Tutorial 9"
author: "Dylan Maciel"
date: "3/11/2020"
linestretch: 1.5
urlcolor: blue
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
``` 


In this week's tutorial we'll be going over the use of indicator variables. 
Also referred to as dummy variables, these are used to incorporate categorical 
predictor variables into the linear regression model. Their typical form is:
\begin{align*}
  x = 
  \begin{cases}
     1 \ \text{if the variable is in a specific category,}\\
     0 \ \text{if the variable is in the other category(s).}
  \end{cases}
\end{align*}
If we think about this in the context of regression, inclusion of this variable 
leads to a difference in the intercept coefficient for the categories. So,
consider a SLR with one dummy variable for a binary predictor:
\begin{align*}
  y = \beta_0 + \beta_1 x.
\end{align*}
The result is a different constant given the value of $x$; if $x=0$, $y=\beta_0$ 
and if $x=1$, $y=(\beta_0 + \beta_1)$. If a variable has $a$ levels it will need 
$a-1$ dummy variables. As the number of categorical variables increases and the 
number of levels for each variable increases, the model specified will become 
more complex. 

Another way to introduce complexity into the model is to add interaction terms 
between predictor variables. The simplest example of which is an interaction 
between a continuous predictor $x_1$and a binary predictor $x_2$. The model 
we'll specify in this case is
\begin{align*}
  y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2.
\end{align*}
Here the fourth term is the interaction term. And, similar to the intercept, the 
value of the slope coefficient can change depending of the level of the binary 
predictor. Specifically, we get two possible equations.
\begin{align*}
 y = 
 \begin{cases}
 \beta_0 + \beta_1 x_1 &\text{if} \ x_2 = 0,\\
 (\beta_0 + \beta_2) + (\beta_1 + \beta_3)x_1 &\text{if} x_2 = 1.
 \end{cases}
\end{align*}

With all these possible models, we'll need to introduce some hypothesis tests.
Derek went over these in lecture: a test for a difference in intercepts 
(parallel lines), a test for difference in slopes (concurrent lines). and a test 
for the difference in the intercepts and slopes (coincident lines). For all of 
these we have a test statistic of the form
\begin{align*}
  F = \frac{[SS_{Res}(RM) - SS_{Res}(FM)]/(df_{RM} - df{FM})}{MS_{Res}(FM)};
\end{align*}
comparing the sum of squared residuals of the reduced model to the sum of 
squared residuals of the full model.

\subsection*{An Example}

For our example we'll again take a dataset from the \verb|faraway| package. The 
\verb|sexab| data consists of data from a study done on women who suffer from 
post-traumatic stress disorder (\verb|ptsd|) with reported childhood sexual 
abuse (\verb|csa|) and childhood physical abuse (\verb|cpa|). \verb|csa| is a 
binary variable while the ither two are continuous. 
```{r}
library(faraway)
data(sexab)
str(sexab)
```
One thing to notice is that \verb|csa| is coded as a factor, if it wasn't we 
would have to change it to this format using the \verb|as.factor()| function 
before our analysis.

It's always a good idea to do some initial EDA before analysis; maybe we can 
find a visual difference between the two groups visually. I made the following 
plots with \verb|ggplot|.

First, we have boxplots for \verb|ptsd| given fir the levels of \verb|csa|. Here
there seems to be a difference between the two groups; we'll test this 
hypothesis later.
```{r, echo=FALSE}
library(tidyverse)
ggplot(sexab) +
  theme_bw() +
  theme(legend.position = 'none') +
  geom_boxplot(aes(x = csa, y = ptsd, fill = csa))
```

Next, we have a scatter plot for \verb|ptsd| given \verb|cpa| where we have set 
the colour to indicate the level of \verb|csa|.
```{r, echo=FALSE}
ggplot(sexab) +
  theme_bw() +
  theme(legend.position = c(0.1, 0.85)) +
  geom_point(aes(x = cpa, y = ptsd, col = csa), size = 2)
```
Here there does seem to be a weak linear relationship between \verb|cpa| and 
\verb|ptsd| and there we also see a stratification where those who were abused 
have higher levels of \verb|ptsd|. You may remember from previous classes that 
you can test the hypothesis for a difference using a t-test.

Next I'll show you how to code dummy variables. We don't actually need to do tis for our dataset as there is only one binary predictor, but you may need this for future assignments. I'll code two dummy variables, one
for those who were sexually abused, and one for those not abused. Here I'll be using the \verb|ifelse()| function to assign new values manually. There are functions from packages that can do this automatically, like \verb|dummyVars| from the \verb|caret| package, that are useful when the number of dummy varibles you'll have is large. 

```{r}
dum1 <- ifelse(sexab$csa == 'Abused', yes = 1, no = 0)
dum2 <- ifelse(sexab$csa == 'NotAbused', yes = 1, no = 0)
```

IF you recall the model specification from above, the model where $x=0$ for the dummy variable is known as the reference level. The choice for this is arbitrary, but it is good practice to choose a reference level that makes sense in the context of the data. For this data the no abuse is the natural choice so I will make sure that \verb|R| uses this a the reference level through the \verb|relevel()| 
function.

```{r}
sexab$csa <- relevel(sexab$csa, ref = 'NotAbused')
```


Once we have are variables definde we can move onto fitting the model. First, we'll fit the full model with interaction. Notice on the right of the \verb|~| I have put \verb|cpa*csa| and the model includes four coefficients.
```{r}
mdl_full <- lm(ptsd ~ cpa*csa, data = sexab)
summary(mdl_full)
```

The first thing to notice in the model summary is that the interaction term is insignificant. So, we'll fit a model without it.
```{r}
mdl_add <- lm(ptsd ~ cpa + csa, data = sexab)
summary(mdl_add)
```

